{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import argparse\n",
    "import pandas as pd \n",
    "\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import gensim\n",
    "import gensim.utils as utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('../data/bgwiki_word2vec/bgwiki_word2vec', mmap='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "девойка 0.458697795868\n",
      "мъжът 0.440020859241\n",
      "проститутка 0.427411824465\n",
      "жената 0.425804615021\n",
      "ревност 0.41505753994\n",
      "девойката 0.408070772886\n",
      "влюбен 0.397029042244\n",
      "целувка 0.395231753588\n",
      "влюбена 0.394356220961\n",
      "усмивка 0.392459958792\n"
     ]
    }
   ],
   "source": [
    "res = model.wv.most_similar_cosmul(positive=[u'жена',u'мъж',u'любов'], negative=[])\n",
    "for k, v in res:\n",
    "    print k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56728704930167073"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(u'любов',u'омраза')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bigram_transformer = gensim.models.Phrases.load('../data/bgwiki_word2vec/bgwiki_word2vec')\n",
    "#model = Word2Vec(bigram_transformer, size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x146a01190>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news = gensim.models.Word2Vec.load('../data/bgwiki_word2vec_news/bgwiki_word2vec_news', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x16a45c410>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Word2Vec in module gensim.models.word2vec object:\n",
      "\n",
      "class Word2Vec(gensim.utils.SaveLoad)\n",
      " |  Class for training, using and evaluating neural networks described in https://code.google.com/p/word2vec/\n",
      " |  \n",
      " |  If you're finished training a model (=no more updates, only querying)\n",
      " |  then switch to the :mod:`gensim.models.KeyedVectors` instance in wv\n",
      " |  \n",
      " |  The model can be stored/loaded via its `save()` and `load()` methods, or stored/loaded in a format\n",
      " |  compatible with the original word2vec implementation via `wv.save_word2vec_format()` and `KeyedVectors.load_word2vec_format()`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2Vec\n",
      " |      gensim.utils.SaveLoad\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Deprecated. Use self.wv.__contains__() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.__contains__`\n",
      " |  \n",
      " |  __getitem__(self, words)\n",
      " |      Deprecated. Use self.wv.__getitem__() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.__getitem__`\n",
      " |  \n",
      " |  __init__(self, sentences=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000)\n",
      " |      Initialize the model from an iterable of `sentences`. Each sentence is a\n",
      " |      list of words (unicode strings) that will be used for training.\n",
      " |      \n",
      " |      The `sentences` iterable can be simply a list, but for larger corpora,\n",
      " |      consider an iterable that streams the sentences directly from disk/network.\n",
      " |      See :class:`BrownCorpus`, :class:`Text8Corpus` or :class:`LineSentence` in\n",
      " |      this module for such examples.\n",
      " |      \n",
      " |      If you don't supply `sentences`, the model is left uninitialized -- use if\n",
      " |      you plan to initialize it in some other way.\n",
      " |      \n",
      " |      `sg` defines the training algorithm. By default (`sg=0`), CBOW is used.\n",
      " |      Otherwise (`sg=1`), skip-gram is employed.\n",
      " |      \n",
      " |      `size` is the dimensionality of the feature vectors.\n",
      " |      \n",
      " |      `window` is the maximum distance between the current and predicted word within a sentence.\n",
      " |      \n",
      " |      `alpha` is the initial learning rate (will linearly drop to `min_alpha` as training progresses).\n",
      " |      \n",
      " |      `seed` = for the random number generator. Initial vectors for each\n",
      " |      word are seeded with a hash of the concatenation of word + str(seed).\n",
      " |      Note that for a fully deterministically-reproducible run, you must also limit the model to\n",
      " |      a single worker thread, to eliminate ordering jitter from OS thread scheduling. (In Python\n",
      " |      3, reproducibility between interpreter launches also requires use of the PYTHONHASHSEED\n",
      " |      environment variable to control hash randomization.)\n",
      " |      \n",
      " |      `min_count` = ignore all words with total frequency lower than this.\n",
      " |      \n",
      " |      `max_vocab_size` = limit RAM during vocabulary building; if there are more unique\n",
      " |      words than this, then prune the infrequent ones. Every 10 million word types\n",
      " |      need about 1GB of RAM. Set to `None` for no limit (default).\n",
      " |      \n",
      " |      `sample` = threshold for configuring which higher-frequency words are randomly downsampled;\n",
      " |          default is 1e-3, useful range is (0, 1e-5).\n",
      " |      \n",
      " |      `workers` = use this many worker threads to train the model (=faster training with multicore machines).\n",
      " |      \n",
      " |      `hs` = if 1, hierarchical softmax will be used for model training.\n",
      " |      If set to 0 (default), and `negative` is non-zero, negative sampling will be used.\n",
      " |      \n",
      " |      `negative` = if > 0, negative sampling will be used, the int for negative\n",
      " |      specifies how many \"noise words\" should be drawn (usually between 5-20).\n",
      " |      Default is 5. If set to 0, no negative samping is used.\n",
      " |      \n",
      " |      `cbow_mean` = if 0, use the sum of the context word vectors. If 1 (default), use the mean.\n",
      " |      Only applies when cbow is used.\n",
      " |      \n",
      " |      `hashfxn` = hash function to use to randomly initialize weights, for increased\n",
      " |      training reproducibility. Default is Python's rudimentary built in hash function.\n",
      " |      \n",
      " |      `iter` = number of iterations (epochs) over the corpus. Default is 5.\n",
      " |      \n",
      " |      `trim_rule` = vocabulary trimming rule, specifies whether certain words should remain\n",
      " |      in the vocabulary, be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |      Can be None (min_count will be used), or a callable that accepts parameters (word, count, min_count) and\n",
      " |      returns either `utils.RULE_DISCARD`, `utils.RULE_KEEP` or `utils.RULE_DEFAULT`.\n",
      " |      Note: The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part\n",
      " |      of the model.\n",
      " |      \n",
      " |      `sorted_vocab` = if 1 (default), sort the vocabulary by descending frequency before\n",
      " |      assigning word indexes.\n",
      " |      \n",
      " |      `batch_words` = target size (in words) for batches of examples passed to worker threads (and\n",
      " |      thus cython routines). Default is 10000. (Larger batches will be passed if individual\n",
      " |      texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True)\n",
      " |  \n",
      " |  build_vocab(self, sentences, keep_raw_vocab=False, trim_rule=None, progress_per=10000, update=False)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      Each sentence must be a list of unicode strings.\n",
      " |  \n",
      " |  clear_sims(self)\n",
      " |      Removes all L2-normalized vectors for words from the model.\n",
      " |      You will have to recompute them using init_sims method.\n",
      " |  \n",
      " |  create_binary_tree(self)\n",
      " |      Create a binary Huffman tree using stored vocabulary word counts. Frequent words\n",
      " |      will have shorter binary codes. Called internally from `build_vocab()`.\n",
      " |  \n",
      " |  delete_temporary_training_data(self, replace_word_vectors_with_normalized=False)\n",
      " |      Discard parameters that are used in training and score. Use if you're sure you're done training a model.\n",
      " |      If `replace_word_vectors_with_normalized` is set, forget the original vectors and only keep the normalized\n",
      " |      ones = saves lots of memory!\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Deprecated. Use self.wv.doesnt_match() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.doesnt_match`\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Deprecated. Use self.wv.evaluate_word_pairs() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.evaluate_word_pairs`\n",
      " |  \n",
      " |  finalize_vocab(self, update=False)\n",
      " |      Build tables and model weights based on final vocabulary settings.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      init_sims() resides in KeyedVectors because it deals with syn0 mainly, but because syn1 is not an attribute\n",
      " |      of KeyedVectors, it has to be deleted in this class, and the normalizing of syn0 happens inside of KeyedVectors\n",
      " |  \n",
      " |  initialize_word_vectors(self)\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge the input-hidden weight matrix from the original C word2vec-tool format\n",
      " |      given, where it intersects with the current vocabulary. (No words are added to the\n",
      " |      existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.)\n",
      " |      \n",
      " |      `binary` is a boolean indicating whether the data is in binary word2vec format.\n",
      " |      \n",
      " |      `lockf` is a lock-factor value to be set for any imported word-vectors; the\n",
      " |      default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |      training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |  \n",
      " |  make_cum_table(self, power=0.75, domain=2147483647)\n",
      " |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      " |      drawing random words in the negative-sampling training routines.\n",
      " |      \n",
      " |      To draw a word index, choose a random integer up to the maximum value in the\n",
      " |      table (cum_table[-1]), then finding that integer's sorted insertion point\n",
      " |      (as if by bisect_left or ndarray.searchsorted()). That insertion point is the\n",
      " |      drawn index, coming up in proportion equal to the increment at that slot.\n",
      " |      \n",
      " |      Called internally from 'build_vocab()'.\n",
      " |  \n",
      " |  most_similar(self, positive=[], negative=[], topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Deprecated. Use self.wv.most_similar() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.most_similar`\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=[], negative=[], topn=10)\n",
      " |      Deprecated. Use self.wv.most_similar_cosmul() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.most_similar_cosmul`\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Deprecated. Use self.wv.n_similarity() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.n_similarity`\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Report the probability distribution of the center word given the context words as input to the trained model.\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures (like vocab) from the other_model. Useful\n",
      " |      if testing multiple models in parallel on the same corpus.\n",
      " |  \n",
      " |  reset_weights(self)\n",
      " |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the object to file (also see `load`).\n",
      " |      \n",
      " |      `fname_or_handle` is either a string specifying the file name to\n",
      " |      save to, or an open file-like object which can be written to. If\n",
      " |      the object is a file handle, no special array handling will be\n",
      " |      performed; all attributes will be saved to the same file.\n",
      " |      \n",
      " |      If `separately` is None, automatically detect large\n",
      " |      numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |      them into separate files. This avoids pickle memory errors and\n",
      " |      allows mmap'ing large arrays back on load efficiently.\n",
      " |      \n",
      " |      You can also set `separately` manually, in which case it must be\n",
      " |      a list of attribute names to be stored in separate files. The\n",
      " |      automatic check is not performed in this case.\n",
      " |      \n",
      " |      `ignore` is a set of attribute names to *not* serialize (file\n",
      " |      handles, caches etc). On subsequent load() these attributes will\n",
      " |      be set to None.\n",
      " |      \n",
      " |      `pickle_protocol` defaults to 2 so the pickled object can be imported\n",
      " |      in both Python 2 and 3.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False)\n",
      " |      Deprecated. Use model.wv.save_word2vec_format instead.\n",
      " |  \n",
      " |  scale_vocab(self, min_count=None, sample=None, dry_run=False, keep_raw_vocab=False, trim_rule=None, update=False)\n",
      " |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      " |      and `sample` (controlling the downsampling of more-frequent words).\n",
      " |      \n",
      " |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      " |      report the size of the retained vocabulary, effective corpus length, and\n",
      " |      estimated memory requirements. Results are both printed via logging and\n",
      " |      returned as a dict.\n",
      " |      \n",
      " |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      " |      unless `keep_raw_vocab` is set.\n",
      " |  \n",
      " |  scan_vocab(self, sentences, progress_per=10000, trim_rule=None)\n",
      " |      Do an initial scan of all words appearing in sentences.\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences (can be a once-only generator stream).\n",
      " |      Each sentence must be a list of unicode strings.\n",
      " |      This does not change the fitted model in any way (see Word2Vec.train() for that).\n",
      " |      \n",
      " |      We have currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with hs=1 and negative=0 for this to work.\n",
      " |      \n",
      " |      Note that you should specify total_sentences; we'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the article by [taddy]_ and the gensim demo at [deepir]_ for examples of how to use such scores in document classification.\n",
      " |      \n",
      " |      .. [taddy] Taddy, Matt.  Document Classification by Inversion of Distributed Language Representations, in Proceedings of the 2015 Conference of the Association of Computational Linguistics.\n",
      " |      .. [deepir] https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb\n",
      " |  \n",
      " |  seeded_vector(self, seed_string)\n",
      " |      Create one 'random' vector (but deterministic by seed_string)\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Deprecated. Use self.wv.similar_by_vector() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.similar_by_vector`\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Deprecated. Use self.wv.similar_by_word() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.similar_by_word`\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Deprecated. Use self.wv.similarity() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.similarity`\n",
      " |  \n",
      " |  sort_vocab(self)\n",
      " |      Sort the vocabulary so the most frequent words have the lowest indexes.\n",
      " |  \n",
      " |  train(self, sentences, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0)\n",
      " |      Update the model's neural weights from a sequence of sentences (can be a once-only generator stream).\n",
      " |      For Word2Vec, each sentence must be a list of unicode strings. (Subclasses may accept other examples.)\n",
      " |      \n",
      " |      To support linear learning-rate decay from (initial) alpha to min_alpha, and accurate\n",
      " |      progres-percentage logging, either total_examples (count of sentences) or total_words (count of\n",
      " |      raw words in sentences) MUST be provided. (If the corpus is the same as was provided to\n",
      " |      `build_vocab()`, the count of examples in that corpus will be available in the model's\n",
      " |      `corpus_count` property.)\n",
      " |      \n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument MUST be provided. In the common and recommended case, where `train()`\n",
      " |      is only called once, the model's cached `iter` value should be supplied as `epochs` value.\n",
      " |  \n",
      " |  update_weights(self)\n",
      " |      Copy all the existing weights, and reset the weights for the newly\n",
      " |      added vocabulary.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Deprecated. Use self.wv.wmdistance() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.wmdistance`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(cls, *args, **kwargs) from __builtin__.type\n",
      " |  \n",
      " |  load_word2vec_format(cls, fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<type 'numpy.float32'>) from __builtin__.type\n",
      " |      Deprecated. Use gensim.models.KeyedVectors.load_word2vec_format instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |      Deprecated. Use self.wv.log_evaluate_word_pairs() instead.\n",
      " |      Refer to the documentation for `gensim.models.KeyedVectors.log_evaluate_word_pairs`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "log_evaluate_word_pairs() takes exactly 4 arguments (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-860806de61d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_evaluate_word_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: log_evaluate_word_pairs() takes exactly 4 arguments (0 given)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
