{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-ef9178ad4956>, line 84)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-ef9178ad4956>\"\u001b[0;36m, line \u001b[0;32m84\u001b[0m\n\u001b[0;31m    small_grid = {\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# %load ../runner/run_learning.py\n",
    "# Thanks to https://github.com/rayidghani/magicloops/blob/master/magicloops.py\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, cross_validation, svm, metrics, tree, decomposition, svm\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, OrthogonalMatchingPursuit, RandomizedLogisticRegression\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "# for jupyter notebooks\n",
    "# %matplotlib inline\n",
    "\n",
    "# if you're running this in a jupyter notebook, print out the graphs\n",
    "NOTEBOOK = 1\n",
    "\n",
    "import time                                                \n",
    "\n",
    "def timeit(method):\n",
    "\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "\n",
    "        print '%r (%r, %r) %2.2f sec' % \\\n",
    "              (method.__name__, args, kw, te-ts)\n",
    "        return result\n",
    "\n",
    "    return timed\n",
    "\n",
    "\n",
    "def define_clfs_params(grid_size):\n",
    "\n",
    "    clfs = {'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1),\n",
    "        'ET': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),\n",
    "        'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\", n_estimators=200),\n",
    "        'LR': LogisticRegression(penalty='l1', C=1e5),\n",
    "        'SVM': svm.SVC(kernel='linear', probability=True, random_state=0),\n",
    "        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10),\n",
    "        'NB': GaussianNB(),\n",
    "        'DT': DecisionTreeClassifier(),\n",
    "        'SGD': SGDClassifier(loss=\"hinge\", penalty=\"l2\"),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=3)\n",
    "            }\n",
    "\n",
    "    large_grid = {\n",
    "    'RF':{'n_estimators': [1, 10, 100, 1000, 10000], 'max_depth': [1, 5, 10, 20, 50, 100], 'max_features': ['sqrt', 'log2'], 'min_samples_split': [2, 5, 10]},\n",
    "    'LR': { 'penalty': ['l1', 'l2'], 'C': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]},\n",
    "    'SGD': { 'loss': ['hinge', 'log', 'perceptron'], 'penalty': ['l2', 'l1', 'elasticnet']},\n",
    "    'ET': { 'n_estimators': [1, 10, 100, 1000, 10000], 'criterion' : ['gini', 'entropy'] , 'max_depth': [1, 5, 10, 20, 50, 100], 'max_features': ['sqrt', 'log2'], 'min_samples_split': [2, 5, 10]},\n",
    "    'AB': { 'algorithm': ['SAMME', 'SAMME.R'], 'n_estimators': [1, 10, 100, 1000, 10000]},\n",
    "    'GB': {'n_estimators': [1, 10, 100, 1000, 10000], 'learning_rate' : [0.001, 0.01, 0.05, 0.1, 0.5], 'subsample' : [0.1, 0.5, 1.0], 'max_depth': [1, 3, 5, 10, 20, 50, 100]},\n",
    "    'NB' : {},\n",
    "    'DT': {'criterion': ['gini', 'entropy'], 'max_depth': [1, 5, 10, 20, 50, 100], 'max_features': ['sqrt', 'log2'], 'min_samples_split': [2, 5, 10]},\n",
    "    'SVM' :{'C' :[0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], 'kernel':['linear']},\n",
    "    'KNN' :{'n_neighbors': [1, 5, 10, 25, 50, 100], 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree']}\n",
    "           }\n",
    "    \n",
    "    semi_large_grid = {\n",
    "    'RF':{'n_estimators': [1, 10, 100, 1000], 'max_depth': [1, 5, 10, 20, 50, 100], 'max_features': ['sqrt', 'log2'], 'min_samples_split': [2, 5, 10]},\n",
    "    'LR': { 'penalty': ['l1', 'l2'], 'C': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]},\n",
    "    'SGD': { 'loss': ['hinge', 'log', 'perceptron'], 'penalty': ['l2', 'l1', 'elasticnet']},\n",
    "    'ET': { 'n_estimators': [1, 10, 100, 1000], 'criterion' : ['gini', 'entropy'] , 'max_depth': [1, 5, 10, 20, 50, 100], 'max_features': ['sqrt', 'log2'], 'min_samples_split': [2, 5, 10]},\n",
    "    'AB': { 'algorithm': ['SAMME', 'SAMME.R'], 'n_estimators': [1, 10, 100, 1000, 10000]},\n",
    "    'GB': {'n_estimators': [1, 10, 100, 1000], 'learning_rate' : [0.001, 0.01, 0.05, 0.1, 0.5], 'subsample' : [0.1, 0.5, 1.0], 'max_depth': [1, 3, 5, 10, 20, 50, 100]},\n",
    "    'NB' : {},\n",
    "    'DT': {'criterion': ['gini', 'entropy'], 'max_depth': [1, 5, 10, 20, 50, 100], 'max_features': ['sqrt', 'log2'], 'min_samples_split': [2, 5, 10]},\n",
    "    'SVM' :{'C' :[0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], 'kernel':['linear']},\n",
    "    'KNN' :{'n_neighbors': [1, 5, 10, 25, 50, 100], 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree']}\n",
    "    \n",
    "\n",
    "    small_grid = {\n",
    "    'RF':{'n_estimators': [10, 100], 'max_depth': [5, 50], 'max_features': ['sqrt', 'log2'], 'min_samples_split': [2, 10]},\n",
    "    'LR': { 'penalty': ['l1', 'l2'], 'C': [0.00001, 0.001, 0.1, 1, 10]},\n",
    "    'SGD': { 'loss': ['hinge', 'log', 'perceptron'], 'penalty': ['l2', 'l1', 'elasticnet']},\n",
    "    'ET': { 'n_estimators': [10, 100], 'criterion' : ['gini', 'entropy'] , 'max_depth': [5, 50], 'max_features': ['sqrt', 'log2'], 'min_samples_split': [2, 10]},\n",
    "    'AB': { 'algorithm': ['SAMME', 'SAMME.R'], 'n_estimators': [1, 10, 100, 1000, 10000]},\n",
    "    'GB': {'n_estimators': [10, 100], 'learning_rate' : [0.001, 0.1, 0.5], 'subsample' : [0.1, 0.5, 1.0], 'max_depth': [5, 50]},\n",
    "    'NB' : {},\n",
    "    'DT': {'criterion': ['gini', 'entropy'], 'max_depth': [1, 5, 10, 20, 50, 100], 'max_features': ['sqrt', 'log2'], 'min_samples_split': [2, 5, 10]},\n",
    "    'SVM' :{'C' :[0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], 'kernel':['linear']},\n",
    "    'KNN' :{'n_neighbors': [1, 5, 10, 25, 50, 100], 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree']}\n",
    "           }\n",
    "\n",
    "    test_grid = {\n",
    "    'RF':{'n_estimators': [1], 'max_depth': [1], 'max_features': ['sqrt'], 'min_samples_split': [10]},\n",
    "    'LR': { 'penalty': ['l1'], 'C': [0.01]},\n",
    "    'SGD': { 'loss': ['perceptron'], 'penalty': ['l2']},\n",
    "    'ET': { 'n_estimators': [1], 'criterion' : ['gini'] , 'max_depth': [1], 'max_features': ['sqrt'], 'min_samples_split': [10]},\n",
    "    'AB': { 'algorithm': ['SAMME'], 'n_estimators': [1]},\n",
    "    'GB': {'n_estimators': [1], 'learning_rate' : [0.1], 'subsample' : [0.5], 'max_depth': [1]},\n",
    "    'NB' : {},\n",
    "    'DT': {'criterion': ['gini'], 'max_depth': [1], 'max_features': ['sqrt'], 'min_samples_split': [10]},\n",
    "    'SVM' :{'C' :[0.01], 'kernel':['linear']},\n",
    "    'KNN' :{'n_neighbors': [5], 'weights': ['uniform'], 'algorithm': ['auto']}\n",
    "           }\n",
    "\n",
    "    if (grid_size == 'large'):\n",
    "        return clfs, large_grid\n",
    "    elif (grid_size == 'semi_large'):\n",
    "        return clfs, semi_large_grid\n",
    "    elif (grid_size == 'small'):\n",
    "        return clfs, small_grid\n",
    "    elif (grid_size == 'test'):\n",
    "        return clfs, test_grid\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "def generate_binary_at_k(y_scores, k):\n",
    "    cutoff_index = int(len(y_scores) * (k / 100.0))\n",
    "    test_predictions_binary = [1 if x < cutoff_index else 0 for x in range(len(y_scores))]\n",
    "    return test_predictions_binary\n",
    "\n",
    "def precision_at_k(y_true, y_scores, k):\n",
    "    preds_at_k = generate_binary_at_k(y_scores, k)\n",
    "    #precision, _, _, _ = metrics.precision_recall_fscore_support(y_true, preds_at_k)\n",
    "    #precision = precision[1]  # only interested in precision for label 1\n",
    "    precision = precision_score(y_true, preds_at_k)\n",
    "    return precision\n",
    "\n",
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score >= value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax2.set_xlim([0, 1])\n",
    "\n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    # plt.savefig(name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "@timeit\n",
    "def clf_loop(models_to_run, clfs, grid, X, y):\n",
    "    results_df = pd.DataFrame(columns=('model_type', 'clf', 'parameters', 'auc-roc', 'p_at_5', 'p_at_10', 'p_at_100'))\n",
    "    for n in range(1, 2):\n",
    "        # create training and valdation sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "        for index, clf in enumerate([clfs[x] for x in models_to_run]):\n",
    "            model_name = models_to_run[index]\n",
    "            print model_name\n",
    "            ts = time.time()\n",
    "            parameter_values = grid[model_name]\n",
    "            for p in ParameterGrid(parameter_values):\n",
    "                try:\n",
    "                    clf.set_params(**p)\n",
    "                    y_pred_probs = clf.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n",
    "                    # you can also store the model, feature importances, and prediction scores\n",
    "                    # we're only storing the metrics for now\n",
    "                    y_pred_probs_sorted, y_test_sorted = zip(*sorted(zip(y_pred_probs, y_test), reverse=True))\n",
    "                    results_df.loc[len(results_df)] = [model_name, clf, p,\n",
    "                                                       roc_auc_score(y_test, y_pred_probs),\n",
    "                                                       precision_at_k(y_test_sorted, y_pred_probs_sorted, 5.0),\n",
    "                                                       precision_at_k(y_test_sorted, y_pred_probs_sorted, 10.0),\n",
    "                                                       precision_at_k(y_test_sorted, y_pred_probs_sorted, 100.0)]\n",
    "                   # if NOTEBOOK == 1:\n",
    "                        #plot_precision_recall_n(y_test, y_pred_probs, clf)\n",
    "                except IndexError, e:\n",
    "                    print 'Error:', e\n",
    "                    continue\n",
    "            te = time.time()\n",
    "            print '%r  %2.2f sec' % (model_name, te-ts)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "\n",
    "def start_from_text():\n",
    "\n",
    "    grid_size = 'test'\n",
    "    clfs, grid = define_clfs_params(grid_size)\n",
    "    #models_to_run = ['RF', 'DT', 'KNN', 'ET', 'AB', 'GB', 'LR', 'NB']\n",
    "\n",
    "    df = pd.read_csv(\"../data/train_data_features.csv\")\n",
    "    df.head(1)\n",
    "    df['fake_news_score_binary'] =  df['fake_news_score'] == 3\n",
    "    features  =  ['title_number_char', 'title_number_stopwords', \n",
    "                  'body_number_char', 'body_number_stopwords', ]\n",
    "\n",
    "    df.head()\n",
    "    X = df[features]\n",
    "    X.head()\n",
    "    y = df.fake_news_score_binary\n",
    "    results_df = clf_loop(models_to_run, clfs, grid, X, y)\n",
    "    if NOTEBOOK == 1:\n",
    "        results_df\n",
    "    results_df.head()\n",
    "    results_df.to_csv('results.csv', index=False)\n",
    "    print \"The End\"\n",
    "\n",
    "def start_from_vector():\n",
    "    pass\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    #start_from_text()\n",
    "    #start_from_vector()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'define_clfs_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5035bc6266b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgrid_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'small'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_clfs_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodels_to_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'RF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KNN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NB'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/train_data_features.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'define_clfs_params' is not defined"
     ]
    }
   ],
   "source": [
    "grid_size = 'small'\n",
    "clfs, grid = define_clfs_params(grid_size)\n",
    "models_to_run = ['RF', 'DT', 'KNN', 'ET', 'GB', 'LR', 'NB']\n",
    "\n",
    "df = pd.read_csv(\"../data/train_data_features.csv\")\n",
    " \n",
    "\n",
    "df['fake_news_score_binary'] =  df['fake_news_score'] == 3\n",
    "features  =  ['title_number_char', 'title_number_stopwords', 'title_number_words', \n",
    "              'title_number_symbols',\n",
    "              'body_number_char', 'body_number_stopwords', 'body_number_words', 'body_avg_char_per_word',\n",
    "              'body_number_symbols']\n",
    "\n",
    "df.head(2)\n",
    "df.fillna(value=0, inplace=True)\n",
    "X = df[features] \n",
    "y = df.fake_news_score_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_df = clf_loop(models_to_run, clfs, grid, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0fe90f8063b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/FN_Training_Set.csv', encoding='windows-1251')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2815, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7756ecc2c267>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "x = np.zeros(10,15,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
